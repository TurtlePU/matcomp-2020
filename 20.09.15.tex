\section{Малоранговое приближение матриц --- 2}

\subsection{Скелетная аппроксимация матриц}

Посмотрим, какие алгоритмы мы уже рассмотрели.

\begin{itemize}
    \item \verb|np.linalg.svd| --- $O(m n \min(n, m))$, при $m = n$ ---
        $O(n^3)$. НО! Гарантированная точность для любой матрицы.
    \item Рандомизированные алгоритмы --- $O(m n r)$ из-за умножения на матрицу.
        Для разреженных матриц сложность ещё меньше. НО! Выигрыш для
        $r \ll \min(m, n)$; не всегда точно.
\end{itemize}

Есть ли алгоритм со сложностью $O(\#\text{ эл-в разложения}) = O((m + n) r)$?

Значит, мы не должны использовать все элементы раскладываемой матрицы $A$.

Скелетное разложение:

\[
    A = C V^T, \quad C \text{ --- базисные столбцы } A;
\]

\begin{center}
    Или:
\end{center}

\[
    A = U R, R \text{ --- базисные строки } A.
\]

\begin{theorem}
    Любая $A \in \CC^{m \times n}$ ранга $r$ представляется в виде

    \[
        A = C \hat{A}^{-1} R
    \]
\end{theorem}

($\hat{A}$ --- любая невырожденная подматрица $r \times r$)

\begin{proof}
    $A = [a_1 \dots a_n]$. Тогда $a_i = C \cdot x_i$. В свою очередь,
    $\hat{a}_i = \hat{A} x_i$.

    \[
        R = [\hat{a}_1 \dots \hat{a}_n] = \hat{A} [x_1 \dots x_n]
        \quad\Rightarrow\quad [x_1 \dots x_n] = \hat{A}^{-1} R;
    \]

    \[
        A = [C x_1 \dots C x_n] = C [x_1 \dots x_n] = C \hat{A}^{-1} R.
    \]
\end{proof}

\begin{theorem}
    Пусть для $A$ существует $B$ ранга $r$: $\norm{A - B}_2 \le \eps$. Пусть
    $\hat{A} \in \CC^{r \times r}$ --- подматрица максимального по модулю
    определителя. Тогда

    \[
        \norm{A - C \hat{A}^{-1} R}_c \le (r + 1) \eps
    \]
\end{theorem}

Также называют CGR-разложением, CUR-разложением или псевдоскелетной
аппроксимацией.

(Дальше по мотивам презы)

Пример --- разложение матрицы Гильберта $a_{ij} = 1 / (i + j - 1)$.

При $r \approx 15$ ошибка внезапно подскакивает. В чём дело? Матрица $\hat{A}$
оказывается близка к вырожденной, а мы её обращаем.

Для устойчивости необходимо регуляризовать вычисление $\hat{A}^{-1}$, например,
с помощью SVD. Для этого в numpy есть функция \verb|np.linalg.pinv|.

Метод неполной крестовой аппроксимации...

(Преза закончилась)

\subsection{ALS алгоритм}

(Alternating least squares / Alternating linear scheme)

Рассмотрим $f: \R^{m \times n} \to \R$. Хотим $f(X) \to \min_{\rank(X) \le r}$.

Например, задача о наилучшем приближении ранга $r$:

\[
    f(X) = \norm{A - X}_F^2;
\]

Или задача matrix completion (пример использования --- рекомендательная
система):

\[
    f(X) = \norm{P_\Omega \circ (A - X)}_F^2,
    \quad (P_\Omega)_{ij} = (i, j) \in \Omega
\]

Можно также $f(X) = \norm{X}_*$.

Вернёмся к алгоритму.

\[
    \min_{\rank{X} \le r} f(X) = \min_{U, V} f(U V^T)
\]

Алгоритм 1: ALS vanilla

\[
    U_{k + 1} = \arg\min_{U} f(U V_k^T);
\]
\[
    V_{k + 1} = \arg\min_{V} f(U_{k + 1} V^T).
\]

У этого алгоритма есть существенные недостатки: вектора одной матрицы начнут
расти, а другой --- уменьшаться; вектора внутри одной матрицы могут становиться
почти линейно зависимыми.

Алгоритм 2: ALS с ортогонализацией

\[
    U := \arg\min_{U} f(U V_k^T);
\]
\[
    U = Q_1 R_1
\]
\[
    V := \arg\min_{V} f(Q_1 V^T);
\]
\[
    V = Q_2 R_2
\]
\[
    U_{k + 1} = Q_1 R_2^T; \quad V_{k + 1} = Q_2
\]

Как решать $f(U V^T) \to \min$, где $U^T V = I$ и $f(X) = \norm{A - U V^T}_F^2$:

\begin{enumerate}
    \item $(A, B)_F = \trace(A^T B)$
    \item $\frac{\partial}{\partial X} \trace(X A) = A^T$
        \begin{proof}
            \[
                \left[ \frac{\partial f}{\partial x_{ij}} \right]_{i,j=1}^{m,n}
                = \frac{\partial f}{\partial X}
            \]
            \[
                \dots
            \]
        \end{proof}
    \item $f(U V^T) = \norm{A - U V^T}_F^2 = (A - U V^T, A - U V^T)_F
        = (A, A) - 2 (A, U V^T) + (U V^T, U V^T) = (A, A) + \dots$
    \item Получаем, что оптимум $V_* = A^T U$, и аналогично $U_* = A V$.
\end{enumerate}
