\section{О курсе}

Большую часть сказанного можно найти в
\href{http://wiki.cs.hse.ru/Матричные_Вычисления_20/21}{вики}.

Правда, кроме указанных на вики источников, было упомянуто ещё два:

\begin{enumerate}
    \item Gilbert (неразборчиво) --- Matrix Methods in Data Science (скорее
        всего, Gilbert Strang --- Matrix Methods in Data Analysis, Signal
        Processing, and Machine Learning)
    \item \href{https://github.com/oseledets}{Ivan Oseledets @ github}. Скорее
        всего, имеются в виду репозитории с названиями \verb|nla20XX|.
\end{enumerate}

\section{Основы матричного анализа}

\subsection{Векторные нормы}

\begin{definition}
    Векторная норма --- функция $f: \mathbb{F}^n \to \R$ такая, что:
    \begin{itemize}
        \item $f(x) \ge 0$;\quad $f(x) = 0 \Leftrightarrow x = 0$;
        \item $f(\alpha x) = |\alpha|\; f(x)$;
        \item $f(x + y) \le f(x) + f(y)$.
    \end{itemize}
\end{definition}

Обозначается $\norm{x}$.

Примеры:

\begin{itemize}
    \item $L_1$-норма (Единичная окружность --- ромб, TODO: нарисовать):
        \[
            \norm{x}_1 = \sum_{i = 1}^n |x_i|
        \]

    \item $L_2$-норма (Единичная окружность --- окружность):
        \[
            \norm{x}_2 = \sqrt{\sum_{i = 1}^n |x_i|^2} = \sqrt{x^* x}
        \]

    \item $A$-норма:
        \[
            \norm{x}_A = \sqrt{x^* A x}, \quad A = A^*,
            \quad \forall x \ne 0: x^* A x > 0
        \]

    \item $L_\infty$-норма (Единичная окружность --- квадрат):
        \[
            \norm{x}_\infty = \max_{1 \le i \le n} |x_i|
        \]

    \item $L_p$-норма:
        \[
            \norm{x}_p = \left( \sum_{i = 1}^n |x_i|^p \right)^{1/p},
            \quad p \ge 1.
        \]
\end{itemize}

\subsubsection{Разреженность в L1-норме}

\[
    A x = b, \quad A \in \R^{m \times n}, \quad m < n
\]

Минимизируем $x$ по $L_2$- и $L_1$-норме, в случае $L_1$ получим
\textbf{разреженное} решение (с большим кол-вом нулей) (TODO: нарисовать).

\subsubsection{Скалярное произведение}

\begin{definition}
    Скалярное произведение $(x, y) = x^* y$.
\end{definition}

\begin{theorem} (Неравенство Коши-Буняковского-Шварца).
    \[
        |(x, y)| \le \norm{x} \cdot \norm{y}.
    \]
\end{theorem}

\begin{theorem} (Неравенство Гёльдера).
    \[
        |(x, y)| \le \norm{x}_p \cdot \norm{y}_q \quad \Leftarrow \quad
        \begin{cases}
            p, q \ge 1; \\
            \frac{1}{p} + \frac{1}{q} = 1.
        \end{cases}
    \]
\end{theorem}

\subsubsection{Унитарная инвариантность L2-нормы}

\begin{definition}
    Унитарная матрица $U$ --- $U \in \CC^{n\times n}$:
    \[
        U^{-1} = U^* \quad (\Leftrightarrow I = U^* U = U U^*)
    \]
\end{definition}

\begin{point}
    Если $U$ --- унитарная матрица, то $\norm{U x}_2 = \norm{x}_2$.
\end{point}

\begin{proof}
    \[
        \norm{U x}_2 = \sqrt{(U x)^* U x} = \sqrt{x^* U^* U x} = \sqrt{x^* x}
        = \norm{x}_2.
    \]
\end{proof}

\subsection{Матричные нормы}

\begin{definition}
    Норма $\norm{\cdot}$ называется матричной, если
    \begin{enumerate}
        \item $\norm{\cdot}$ --- векторная норма на пространстве матриц;
        \item $\norm{A B} \le \norm{A} \cdot \norm{B}$ (субмультипликативность).
    \end{enumerate}
\end{definition}

Примеры:

\begin{itemize}
    \item Норма Фробениуса:
        \[
            \norm{A}_F = \sqrt{\sum_{i = 1}^m \sum_{j = 1}^n |a_{i j}|^2}
            = \sqrt{trace(A^* A)}.
        \]
    \item Операторные нормы. Если $\norm{\cdot}_*$,\; $\norm{\cdot}_{**}$
        --- векторные нормы, то соответствующей им операторной нормой будет
        \[
            \norm{A}_{*, **} = \sup_{x \ne 0} \frac{\norm{A x}_*}{\norm{x}_{**}}
            = \sup_{\norm{y}_{**} = 1} \norm{A y}_{*}.
        \]
    \item Например, операторной нормой, соответствующей $L_2$-норме, является
        \[
            \norm{A}_2 = \sqrt{\lambda_{max}(A^* A)} = \sigma_1(A).
        \]
\end{itemize}

\begin{point}
    Для любой матрицы $A$ и для любых унитарных матриц $U, V$ верно
    \[
        \begin{array}{c}
            \norm{U A V}_F = \norm{A}_F \\
            \norm{U A V}_2 = \norm{A}_2
        \end{array}
    \]
\end{point}

\begin{proof}
    Для $\norm{\cdot}_2$:
    \[
        \norm{U A V}_2 = \sup_{x \ne 0} \frac{\norm{U A V x}_2}{\norm{x}_2}
        = \sup_{x \ne 0} \frac{\norm{U^*(U A V x)}_2}{\norm{V x}_2}
    \]

    Заменим $V x$ на $y$. В силу обратимости $V$ это будет равно
    \[
        \sup_{y \ne 0} \frac{\norm{A y}_2}{\norm{y}_2} = \norm{A}_2.
    \]
\end{proof}

\subsection{Разложение Шура}

Собственное разложение (существует не всегда):

\[
    A = S \Lambda S^{-1}, \quad \Lambda = diag(\lambda_1, \dots, \lambda_n)
\]

Жорданова форма (всегда существует, но неустойчива при вычислениях):

\[
    A = P J P^{-1}
\]

Для вычислений используют разложение Шура.

\begin{theorem}
    (О разложении Шура)

    Для всякой $A \in \CC^{n \times n}$ существуют такие унитарная $U$ и
    верхнетреугольная $T$, что $A = U T U^*$.
\end{theorem}

\begin{proof} Индукцией по размерности $A$.
    \begin{base} $n = 1$: $U = I$, $T = A$. \end{base}
    \begin{induct}
        $(n - 1) \to n$.

        Т.к. $\CC$ алгебраически замкнуто, у характеристического многочлена $A$
        есть хотя бы один корень, т.е. у $A$ есть хотя бы одно собственное
        значение $\lambda_1$, т.е. всегда найдётся хотя бы один ненулевой
        собственный вектор $v_1$ единичной длины.

        Дополним $v_1$ до ортонормированного базиса $v_1, \dots , v_n$ и положим
        $U_1 = ( v_1 | \dots | v_n )$.

        $v_1$ --- собственный вектор, так что $A v_1 = \lambda_1 v_1$. Тогда, в
        силу ортогональности $v_i$ и $v_j$,
        \[
            v_i^* A v_1 = \begin{cases}
                \lambda, & i = 1; \\
                0, & i \ne 1.
            \end{cases}
        \]

        Поумножаем пару
        матриц:

        \[
            U_1^* A U_1 = \begin{pmatrix} v_1^* \\ \vdots \\ v_n^* \end{pmatrix}
            \cdot \begin{pmatrix} A v_1 & \dots & A v_n \end{pmatrix}
            = \begin{pmatrix}
                \lambda & v_1^* A v_2 & \dots \\
                0 \\
                \vdots & A_1 \\
                0
            \end{pmatrix}
        \]

        По индукции разложим $A_1$ как $V_1 T_1 V_1^*$. Запишем $U_1^* A U_1$ с
        помощью блочного умножения:

        \[
            \begin{pmatrix} 1 & 0 \\ 0 & V_1 \end{pmatrix}
            \cdot \begin{pmatrix} \lambda & \dots \\ 0 & T_1 \end{pmatrix}
            \cdot \begin{pmatrix} 1 & 0 \\ 0 & V_1^* \end{pmatrix}
        \]

        В силу обратимости $V_1^*$ мы можем так сделать (иначе вектора-строки
        для $\dots$ над $T_1$ могло бы и не существовать).

        Получаем, что

        \[
            \begin{array}{c}
                T = \begin{pmatrix}
                    \lambda & \dots \\
                    0 & T_1
                \end{pmatrix}; \\
                U = U_1 \cdot \begin{pmatrix}
                    1 & 0 \\
                    0 & V_1
                \end{pmatrix}.
            \end{array}
        \]

        Действительно, $T$ верхнетреугольная по построению, а $U$ унитарна как
        произведение двух унитарных матриц.
    \end{induct}
\end{proof}

\subsection{Нормальные матрицы}

\begin{definition}
    Матрица $A$ называется нормальной, если $A^* A = A A^*$.
\end{definition}

\begin{point}
    Матрица диагонализуема в унитарном базисе тогда и только тогда, когда она
    является нормальной.
\end{point}

\begin{proof}.

    \begin{itemize}
        \item $(\Rightarrow)$:
            \[
                A^* A = U \Lambda^* U^* U \Lambda U^* = U \Lambda^* \Lambda U^*
                = U \Lambda \Lambda^* U^* = A A^*.
            \]
        \item $(\Leftarrow)$: Разложение Шура для $A$: $U T U^*$.
            \[
                A^* A = A A^* \Rightarrow T^* T = T T^*.
            \]
            Оставшаяся часть доказательства $(\Leftarrow)$ будет в качестве
            упражнения в ДЗ.
    \end{itemize}
\end{proof}

\end{document}
